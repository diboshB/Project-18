{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19f7c76-9fb1-449a-a663-d76e5a8e3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original coding process\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# File path for the RBI documents\n",
    "pdf_paths = [\n",
    "    \"/Users/diboshbaruah/Capstone_Project - Generative AI/financial risk.pdf\",\n",
    "    \"/Users/diboshbaruah/Capstone_Project - Generative AI/operations risk.pdf\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45681cff-664e-4a2d-b62a-342b9253d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the RecursiveCharacterTextSplitter to break down text into smaller chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Splitting the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09b7f779-3b5d-4cf6-be21-6828de391e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating embeddings for each document chunk using DistilBERT\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Loading the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Converting the documents to embeddings directly\n",
    "doc_embeddings = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Tokenizing the document text\n",
    "    inputs = tokenizer(doc.page_content, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Getting the model outputs without calculating gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Getting the mean of the last hidden state (this gives us the embedding)\n",
    "    doc_embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "\n",
    "# Converting the list of embeddings to a NumPy array\n",
    "doc_embeddings_np = np.array(doc_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6f0b7f-e129-4a33-90c8-35127973576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hugging Faceâ€™s all-MiniLM-L6-v2 to create embeddings for document content\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Initializing the HuggingFaceEmbeddings model with a sentence-transformers model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"This is a financial risk management document.\"),\n",
    "    Document(page_content=\"This is an operational risk management document.\"),\n",
    "]\n",
    "\n",
    "# Creating a FAISS vector store with the documents and embeddings model\n",
    "vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Saving the FAISS index \n",
    "vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1535d9c5-bc9e-47a8-aefd-b8a4839792a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.14525197446346283, 'start': 10, 'end': 93, 'answer': 'financial risk management document. This is an operational risk management document'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Extract context from document chunks\n",
    "context = \" \".join([doc.page_content for doc in docs])  # Combine all chunks into a single context\n",
    "\n",
    "max_context_length = 1000\n",
    "\n",
    "if len(context.split()) > max_context_length:\n",
    "    summarized_context = summarizer(context, max_length=300, min_length=100, do_sample=False)[0]['summary_text']\n",
    "else:\n",
    "    summarized_context = context\n",
    "\n",
    "# 6. Question-Answering Pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "query = \"How loans are processed?\"\n",
    "\n",
    "answer = qa_pipeline(question=query, context=summarized_context)\n",
    "\n",
    "# Print the Answer\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842ce93a-413a-4440-8a3f-d61e318480e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Creating the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Defining a Pydantic model for the query request\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7af3f813-e99d-4450-ada3-10e96a2c0c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import threading\n",
    "import uvicorn\n",
    "import logging\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Updated import\n",
    "\n",
    "# Applying nest_asyncio to allow FastAPI to run on Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Suppress all logs at the INFO level and below for relevant loggers\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# Suppress Uvicorn logs\n",
    "logging.getLogger(\"uvicorn\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"uvicorn.error\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress any other third-party library logs (e.g., FAISS, Hugging Face transformers)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)  # Example for Hugging Face\n",
    "logging.getLogger(\"faiss\").setLevel(logging.WARNING)  # Example for FAISS\n",
    "\n",
    "# Initializing the Hugging Face QA pipeline with a fine-tuned model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Creating an embedding object using HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Loading the FAISS vector store (Make sure the path is correct where your index is saved)\n",
    "vector_store = FAISS.load_local(\"./faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Creating FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Defining a Pydantic model for the query request\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "# Defining the FastAPI endpoint for question answering\n",
    "@app.post(\"/ask-question/\")\n",
    "async def ask_question(query_request: QueryRequest):\n",
    "    query = query_request.question\n",
    "\n",
    "    # Performing similarity search on the FAISS vector store \n",
    "    search_results = vector_store.similarity_search(query, k=1)  \n",
    "\n",
    "    # Only taking the top chunk as context\n",
    "    context = search_results[0].page_content  \n",
    "    \n",
    "    # Using the QA pipeline to generate an answer based on the context\n",
    "    answer = qa_pipeline(question=query, context=context)\n",
    "\n",
    "    # Returning the answer in the response\n",
    "    return {\"answer\": answer['answer']}\n",
    "\n",
    "\n",
    "# Function to run the FastAPI app in a separate thread\n",
    "def run_fastapi():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8007, log_level=\"info\")\n",
    "\n",
    "# Running the FastAPI app in a separate thread\n",
    "thread = threading.Thread(target=run_fastapi)\n",
    "thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "819c8b94-81a3-4608-bbb2-5d2827d88e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings  # Updated import\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Loading documents and create embeddings\n",
    "pdf_paths = [\"/Users/diboshbaruah/Capstone_Project - Generative AI/financial risk.pdf\",\n",
    "    \"/Users/diboshbaruah/Capstone_Project - Generative AI/operations risk.pdf\"]\n",
    "documents = []\n",
    "\n",
    "for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Splitting documents into smaller chunks for better similarity matching\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Using HuggingFace embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Creating the FAISS vector store\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Saving the FAISS index to disk\n",
    "vector_store.save_local(\"./faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b8d2162-e4f8-4c84-8728-360c187be50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:64363 - \"POST /ask-question/ HTTP/1.1\" 200 OK\n",
      "\n",
      "Answer: Loan Review Mechanism\n"
     ]
    }
   ],
   "source": [
    "# Sending HTTP POST requests to the FastAPI server\n",
    "\n",
    "import requests\n",
    "\n",
    "# The URL of the FastAPI server\n",
    "url = \"http://127.0.0.1:8007/ask-question/\"\n",
    "\n",
    "# Defining the question payload\n",
    "question_payload = {\n",
    "    \"question\": \"How loans are processed?\"\n",
    "}\n",
    "\n",
    "# Sending a POST request to the FastAPI server\n",
    "response = requests.post(url, json=question_payload)\n",
    "\n",
    "print()\n",
    "\n",
    "# Checking the response status code and output the answer\n",
    "if response.status_code == 200:\n",
    "    answer = response.json()  \n",
    "    print(f\"Answer: {answer['answer']}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74047e-440f-4630-bdd6-be6e6dcb7a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52c24cfe-3143-432f-b624-e8c627dc6a0b",
   "metadata": {},
   "source": [
    "** Now running the saved scripts on jupyter notebook - embedding.py // app.py // client.py ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5759a8-18aa-4650-8689-21e34814a708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a74ce0bf-6a16-4ca5-8e48-72ab3cacc124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and FAISS index creation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [36620]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 48] error while attempting to bind on address ('127.0.0.1', 8007): address already in use\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:64385 - \"POST /ask-question/ HTTP/1.1\" 200 OK\n",
      "\n",
      "Answer: Loan Review Mechanism\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "# Running embedding.py\n",
    "%run embedding.py\n",
    "\n",
    "# Running app.py to start the FastAPI server\n",
    "%run app.py\n",
    "\n",
    "# Running client.py to test the FastAPI API call\n",
    "%run client.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5522bd-5018-466e-b0c0-1498184498b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663792bf-f5c7-40ba-9d67-fe6cc73e8f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
